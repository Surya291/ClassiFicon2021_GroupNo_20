{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Feature Generation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eP0vO6fzpFcj"
      },
      "source": [
        "# Contest\n",
        "\n",
        "ClassiFicon Hackathon 2021 by **Iâ€™mbesideyou Inc.**\n",
        "\n",
        "Face Expression Detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XE7Eav-cpHPV"
      },
      "source": [
        "### Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOy-YPgepGx7"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as pimg\n",
        "import cv2\n",
        "import imutils\n",
        "from imutils import face_utils\n",
        "import dlib\n",
        "import os\n",
        "from zipfile import ZipFile\n",
        "\n",
        "# Importing Deep Learning Libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgkBDYy1qlnu"
      },
      "source": [
        "Connecting Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hE3qaScmp8eW",
        "outputId": "1b9ba14d-142d-47ac-dc49-c9d8efc535c5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive/My\\ Drive/Contest"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n",
            "/gdrive/My Drive/Contest\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qk2yQLffptZ9"
      },
      "source": [
        "### Importing Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cu8UL85U4R74"
      },
      "source": [
        "Unzip Images\n",
        "\n",
        "```\n",
        "with ZipFile('happy_images.zip', 'r') as zipObj:\n",
        "   zipObj.extractall()\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jz5HH-LCweCG"
      },
      "source": [
        "with ZipFile('happy_images.zip', 'r') as zipObj:\n",
        "   zipObj.extractall()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2hfDC7Bqnzy"
      },
      "source": [
        "Extracting Labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9OxYn4kpvGt"
      },
      "source": [
        "# Train Labels\n",
        "df_Train = pd.read_csv(\"train.csv\", names=['Image', 'Label'])\n",
        "Train_Images = df_Train['Image'].values\n",
        "y_Train = df_Train['Label'].values\n",
        "\n",
        "# Test Labels\n",
        "df_Valid = pd.read_csv(\"test.csv\", names=['Image', 'Label'])\n",
        "Valid_Images = df_Valid['Image'].values\n",
        "y_Valid = df_Valid['Label'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VB9CTufsreca"
      },
      "source": [
        "Map Labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRrTORFhrfj6"
      },
      "source": [
        "def EncodeLabels(Labels):\n",
        "    LabelMap = {\"NOT smile\":[1,0,0], \"positive smile\":[0,1,0], \"negative smile\":[0,0,1]}\n",
        "    y = []\n",
        "\n",
        "    for i in range(Labels.shape[0]):\n",
        "        y.append(LabelMap[Labels[i]])\n",
        "\n",
        "    return np.array(y)\n",
        "\n",
        "y_Train = EncodeLabels(y_Train)\n",
        "y_Valid = EncodeLabels(y_Valid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzO6vqQlrMHg"
      },
      "source": [
        "Extracting Images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQs4SOIBrFzg"
      },
      "source": [
        "def ExtractImages(ImageNames):\n",
        "    X = []\n",
        "    for i in range(ImageNames.shape[0]):\n",
        "        img = pimg.imread(\"happy_images/\" + ImageNames[i] + \".jpg\")\n",
        "        img = cv2.resize(img,(192,192))\n",
        "        X.append(img)\n",
        "\n",
        "    return np.array(X)\n",
        "\n",
        "X_Train = ExtractImages(Train_Images)\n",
        "X_Valid = ExtractImages(Valid_Images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bq_WyUFE51AU"
      },
      "source": [
        "### Preprocessing Images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2ennhZN7KQF"
      },
      "source": [
        "Display Images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJH3Yw9K7NG7"
      },
      "source": [
        "def DisplayImages(Data):\n",
        "    N = Data.shape[0]\n",
        "    i = np.random.randint(N)\n",
        "\n",
        "    plt.figure(figsize=(6,8))\n",
        "    plt.imshow(Data[i])\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xrbwlVi54Wk"
      },
      "source": [
        "Contrast Correction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_L8nOPj5-bn"
      },
      "source": [
        "def ContrastCorrection(Dataset):\n",
        "    Data = []\n",
        "    for i in range(Dataset.shape[0]):\n",
        "        r_image, g_image, b_image = cv2.split(Dataset[i])\n",
        "        r_image_eq = cv2.equalizeHist(r_image)\n",
        "        g_image_eq = cv2.equalizeHist(g_image)\n",
        "        b_image_eq = cv2.equalizeHist(b_image)\n",
        "        image_eq = cv2.merge((r_image_eq, g_image_eq, b_image_eq))\n",
        "        Data.append(image_eq)\n",
        "\n",
        "    return np.array(Data)\n",
        "\n",
        "X_Train = ContrastCorrection(X_Train)\n",
        "X_Valid = ContrastCorrection(X_Valid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJKfwk-eItJ_"
      },
      "source": [
        "### Key Point Detection\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ef4znvu_KikA"
      },
      "source": [
        "def LandMarks(image):\n",
        "    Regions = []\n",
        "\n",
        "    detector = dlib.get_frontal_face_detector()\n",
        "    predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
        "    image = imutils.resize(image, width=500)\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    rects = detector(gray, 1)\n",
        "    for (i, rect) in enumerate(rects):\n",
        "        shape = predictor(gray, rect)\n",
        "        shape = face_utils.shape_to_np(shape)\n",
        "        for (name, (i, j)) in face_utils.FACIAL_LANDMARKS_IDXS.items():\n",
        "            clone = image.copy()\n",
        "            cv2.putText(clone, name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                0.7, (0, 0, 255), 2)\n",
        "            for (x, y) in shape[i:j]:\n",
        "                cv2.circle(clone, (x, y), 1, (0, 0, 255), -1)\n",
        "            (x, y, w, h) = cv2.boundingRect(np.array([shape[i:j]]))\n",
        "            roi = image[y:y + h, x:x + w]\n",
        "            roi = imutils.resize(roi, width=250, inter=cv2.INTER_CUBIC)\n",
        "            \n",
        "            roi = cv2.resize(roi,(150,100))\n",
        "            Regions.append(roi)\n",
        "\n",
        "    return np.array(Regions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eq3sH3SSR__Y"
      },
      "source": [
        "### Region Extraction\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YiJuShS9L90I"
      },
      "source": [
        "def Extraction(Images,Labels):\n",
        "    X,y = [],[]\n",
        "    for i in range(Images.shape[0]):\n",
        "        print (i)\n",
        "        try:\n",
        "            Regions = LandMarks(Images[i])\n",
        "            if Regions.shape != (8,100,150,3):\n",
        "                img = cv2.resize(Images[i],(150,100))\n",
        "                Regions = np.repeat(np.expand_dims(img,axis=0),8,axis=0)\n",
        "            X.append(Regions)\n",
        "            y.append(Labels[i])\n",
        "        except:\n",
        "            img = cv2.resize(Images[i],(150,100))\n",
        "            Regions = np.repeat(np.expand_dims(img,axis=0),8,axis=0)\n",
        "            X.append(Regions)\n",
        "            y.append(Labels[i])\n",
        "\n",
        "    return np.array(X),np.array(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXIYtg_5vE6U"
      },
      "source": [
        "X_TrainFeatures,y_Train = Extraction(X_Train,y_Train)\n",
        "X_ValidFeatures,y_Valid = Extraction(X_Valid,y_Valid)\n",
        "\n",
        "np.save(\"XTrain.npy\", X_Train)\n",
        "np.save(\"XValid.npy\", X_Valid)\n",
        "np.save(\"XTrainFeatures.npy\", X_TrainFeatures)\n",
        "np.save(\"XValidFeatures.npy\", X_ValidFeatures)\n",
        "\n",
        "np.save(\"yTrain.npy\", y_Train)\n",
        "np.save(\"yValid.npy\", y_Valid)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}